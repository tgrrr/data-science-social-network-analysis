{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# TODO:\n",
        "# Make nodes bigger/another colour\n",
        "\n",
        "# ----------------------------------------------------------------------------\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# COSC2671 Social Media and Network Analytics\n",
        "## Assignment 2\n",
        "### @author Phil Steinke, student: s3725547 github: tgrrr\n",
        "### @author Oliver Eaton, student: s3641518 github: shaggycamel\n",
        "### @author FIXME Aidan Cowie, student: s3429481 github: aidancowie\n",
        "### Due FIXME\n",
        "\n",
        "----------------------------------------------------------------------------\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Introduction\n",
        "## Describe what/who is your selected entity\n",
        "## Describe why it is interesting to find the sentiment and topics (answer questions) of this entity\n",
        "\n",
        "#### RUBRIC: Problem Formulation (15%)\n",
        "- [ ] The proposed problem/question is interesting, well-motivated and non-trivial. \n",
        "- [ ] Problem and success criteria are well designed and thought out, \n",
        "  - [ ] and scope is realistic.\n",
        "\n",
        "## Problem or Question Construction\n",
        "\n",
        "- [ ] Must include at least one source of social media and networks.\n",
        "- [ ] Must include a graph or network and its analysis/processing to solve a problem.\n",
        "- [ ] Must include some element of analysis using social media and networks, and something we learnt in class. It cannot be all machine learning for example.\n",
        "- [ ] Your analyst or solution is in Python (no R, including visualisation), but parts of it could be done in other languages, e.g., Javascript if you decide to prototype a front end on website.\n",
        "- [ ] All code should be your own, you can leverage packages such as `networkx` etc., but it shouldnâ€™t be copied from an existing solution.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Data Collection\n",
        "## Describe how you collected the data, and briefly why you chose that approach (restful vs stream)\n",
        "## Report some statistics of your collected data\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "PATH = '/Users/phil/code/data-science-next/uni/social-media/ass02_datascience_social_network_analysis/packages/server'\n",
        "os.chdir(PATH)\n",
        "\n",
        "from src.data import * # fetch_github_query, Auth.HEADERS\n",
        "from src.data.auth import * # fetch_github_query, Auth.HEADERS\n",
        "from src.config import * # Constants, including URL, PATH\n",
        "from src.data.queries import queries\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What it includes:\n",
        "\n",
        "- github api v4 \n",
        "- graphql queries\n",
        "- python 3.7\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "result_charity = do_fetch_github_query(queries.query_repo, queries.variables_repo)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "How many repos:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "repos_charity_count = result_charity['search']['repositoryCount']\n",
        "print(repos_charity_count)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What are the top repos?\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "repos_charity = result_charity['search']['edges']\n",
        "\n",
        "# import json\n",
        "for i in repos_charity:\n",
        "    _name = i['node']['name']\n",
        "    _url = i['node']['url']\n",
        "    _stars = str(i['node']['stargazers']['totalCount'])\n",
        "    print(\"Name: %(n)s \\nstars: %(s)s \\nurl: %(u)s\\n\" \n",
        "        % {'n': _name, 'u': _url, 's': _stars})\n",
        "    # print(json.dumps(i))\n",
        "\n",
        "# get values from repo results for #1\n",
        "# for key, value in repos_charity[1]['node'].items():\n",
        "#     print(key, value)\n",
        "\n",
        "# more concise code:\n",
        "# repo_arr = [i['node']['name'] for i in repos_charity]\n",
        "# repo_arr\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# TODO: Pre-processing and Data Cleaning\n",
        "## TODO: Describe  what pre-processing you performed\n",
        "## TODO: Show examples of noisy data, plot some graphs, etc to show why you decided to do those pre-processing\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to obj to make data easier to access:\n",
        "# FIXME:\n",
        "from collections import namedtuple\n",
        "def _json_object_hook(d): return namedtuple('X', d.keys())(*d.values())\n",
        "def json2obj(data): return json.loads(data, object_hook=_json_object_hook)\n",
        "result_charity_obj = json2obj(json.dumps(result_charity))\n",
        "# dir(result_charity_obj.search.edges[1])\n",
        "\n",
        "# before:\n",
        "result_charity['search']['repositoryCount']\n",
        "#after:\n",
        "result_charity_obj.search.repositoryCount\n",
        "\n",
        "result_charity_obj.search.edges[1].node.stargazers.totalCount\n",
        "\n",
        "# Have we gone over the query count?\n",
        "\n",
        "result_rate_limit = fetch_github_query(queries.query_rate_limit) # Execute the query\n",
        "remaining_rate_limit = result_rate_limit[\"rateLimit\"][\"remaining\"] # Drill down the dictionary\n",
        "print(\"Remaining rate limit - {}\".format(remaining_rate_limit))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP: Query about charities\n",
        "# see: https://developer.github.com/v4/explorer/\n",
        "# queries: https://developer.github.com/v4/query/\n",
        "\n",
        "# FIXME: KeyError: data\n",
        "result_nonprofit = fetch_github_query(queries.query_nonprofit)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Goal:**  Show a network of developers that are contributing to open source projects\n",
        "\n",
        "- ggplot2\n",
        "- spark\n",
        "- tidyverse\n",
        "- linux\n",
        "- django\n",
        "- keras\n",
        "- pandas\n",
        "- ipython\n",
        "- scikit-learn\n",
        "- node\n",
        "- tensorflow\n",
        "- TODO: FreeCodeCamp https://github.com/freeCodeCamp/freeCodeCamp\n",
        "- TODO: Linux\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from os import listdir\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import altair as alt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Get list of json files in directory\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/Users/phil/code/data-science-next/uni/social-media/ass02_datascience_social_network_analysis/packages/server/data/raw'\n",
        "FIGURES_PATH = '/Users/phil/code/data-science-next/uni/social-media/ass02_datascience_social_network_analysis/packages/server/notebooks/figures/'\n",
        "\n",
        "os.chdir(PATH)\n",
        "\n",
        "# Set default figure size for plots:\n",
        "matplotlib.rc('figure', figsize=(10, 10))\n",
        "\n",
        "# From Oli's Jupyter notebook:\n",
        "# Get list of json files in directory\n",
        "files_in_drive = listdir(PATH)\n",
        "json_files = []\n",
        "for ix, file in enumerate(files_in_drive):\n",
        "    if('.json' in file):\n",
        "        json_files.append(file)\n",
        "   \n",
        "# Remove files if wanted\n",
        "rm = []\n",
        "# rm.append(['linux', 'ipython', 'spark', 'node']) # cosc\n",
        "# rm.append(['django', 'keras', 'tensorflow', 'pandas', 'scikit-learn']) # Python\n",
        "# rm.append(['ggplot2', 'tidyverse']) # R\n",
        "for layer in rm:\n",
        "    for f in layer:\n",
        "        json_files.remove(f+\".json\")\n",
        "json_files\n",
        "\n",
        "# Prepare keys for dictionary\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare keys for dictionary\n",
        "file_names = []\n",
        "for file in json_files:\n",
        "    start = file.find('.')\n",
        "    file_names.append(file[0:start])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Put all json data into dictionary\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data = {}\n",
        "for ix, file in enumerate(json_files):\n",
        "    tmp = json.loads(open(file, encoding=\"utf-8\").read())\n",
        "    data[file_names[ix]] = tmp['data']['repository']['ref']['target']['history']['edges']\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph with weight\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Helpers\n",
        "repo_user_list = {}\n",
        "repo_weight = {}\n",
        "\n",
        "for repo in data:\n",
        "    ix = 0\n",
        "    user_list = []\n",
        "    weight = []\n",
        "    while ix < len(data[repo]):\n",
        "        try:\n",
        "            user = data[repo][ix]['node']['author']['user']['login']\n",
        "            weight.append(user)\n",
        "            user_list.append(user)\n",
        "        except:\n",
        "            pass\n",
        "        ix += 1\n",
        "    repo_user_list[repo] = list(set(user_list))\n",
        "    repo_weight[repo] = Counter(weight)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top contributors for each repo\\n\")\n",
        "for repo in repo_weight:\n",
        "    print(repo + \": \" + str(repo_weight[repo].most_common(5)))\n",
        "    # TODO: pretty print"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of unique contributors for each Repo\\n\")\n",
        "for repo in repo_weight:\n",
        "    print(repo + \": \" + str(len(repo_weight[repo])))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_contributions = {}\n",
        "\n",
        "for repo in repo_user_list:\n",
        "    for user in repo_user_list[repo]:\n",
        "        if (user not in user_contributions):\n",
        "            user_contributions[user] = str(repo)\n",
        "        else:\n",
        "            tmp = str(user_contributions[user] + \" \" + repo).split(\" \")\n",
        "            user_contributions[user] = tmp\n",
        "\n",
        "print(\"Coders who contribute to more than one repo\\n\")\n",
        "for user in user_contributions:\n",
        "    if type(user_contributions[user]) != str:\n",
        "        print(user + \": \" + str(user_contributions[user]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_contributions = Counter()\n",
        "for repo in repo_weight:\n",
        "    for user in repo_weight[repo]:\n",
        "        number_of_contributions[user] += repo_weight[repo][user]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network_weight = nx.Graph()\n",
        "\n",
        "for repo in repo_weight:\n",
        "    size = sum(repo_weight[repo].values())\n",
        "    network_weight.add_node(repo, repo=repo, size=size, l=repo)\n",
        "\n",
        "for user in user_contributions:\n",
        "    repo = user_contributions[user]\n",
        "    if type(repo) == str:\n",
        "#         size = number_of_contributions[user]\n",
        "        weight = repo_weight[repo][user]\n",
        "        network_weight.add_node(user, repo=repo)\n",
        "        network_weight.add_edge(repo, user, weight=weight)\n",
        "    else:\n",
        "#         size = number_of_contributions[user]\n",
        "        network_weight.add_node(user, repo='multiple', size = 20, l=user)\n",
        "        for contribution in repo:              \n",
        "            weight = repo_weight[contribution][user]\n",
        "            network_weight.add_edge(contribution, user, weight=weight)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network_weight = nx.Graph()\n",
        "\n",
        "for repo in repo_weight:\n",
        "    network_weight.add_node(repo, repo=repo)\n",
        "    for user in repo_weight[repo]:\n",
        "        if (network_weight.has_node(user)) == False:\n",
        "            network_weight.add_node(user, repo='multiple')\n",
        "            network_weight.add_edge(repo, user, weight=repo_weight[repo][user])\n",
        "        else:\n",
        "            network_weight.add_node(user, repo=repo)\n",
        "            network_weight.add_edge(repo, user, weight=repo_weight[repo][user])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# degree centrality\n",
        "lDegCentrality = nx.degree_centrality(network_weight)\n",
        "for nodeId, cent in lDegCentrality.items():\n",
        "    network_weight.node[nodeId]['degree'] = float(cent)\n",
        "    \n",
        "# eigenvector centrality\n",
        "# eigen vector centrality make sno sense on bi-partite graphs\n",
        "# https://stackoverflow.com/questions/43208737/using-networkx-to-calculate-eigenvector-centrality?rq=1\n",
        "lEigenVectorCentrality = nx.eigenvector_centrality_numpy(network_weight)\n",
        "for nodeId, cent in lEigenVectorCentrality.items():\n",
        "    network_weight.node[nodeId]['eigen'] = float(cent)\n",
        "    \n",
        "# katz centrality\n",
        "lKatzCentrality = nx.katz_centrality(network_weight)    \n",
        "for nodeId, cent in lKatzCentrality.items():\n",
        "    network_weight.node[nodeId]['katz'] = float(cent)\n",
        "\n",
        "# Betweeness centrality\n",
        "lBetCentrality = nx.betweenness_centrality(network_weight)\n",
        "for nodeId, cent in lBetCentrality.items():\n",
        "    network_weight.node[nodeId]['betweenness'] = float(cent)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for g in nx.local_bridges(network_weight, with_span=True, weight=None):\n",
        "    if g[2] < 100:\n",
        "        print(g)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "alt.renderers.enable('notebook')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Data\n",
        "source = pd.DataFrame({\n",
        "    'Degree Centrality': list(lDegCentrality.values()),\n",
        "#     'Eigenvector Centrality': list(lEigenVectorCentrality.values()),\n",
        "    'Betweenness Centrality': list(lBetCentrality.values()),\n",
        "    'Katz Centrality': list(lKatzCentrality.values())\n",
        "})\n",
        "\n",
        "chart = alt.Chart(source).transform_fold(\n",
        "    ['Degree Centrality', 'Betweenness Centrality', 'Katz Centrality'],\n",
        "    as_=['Centrality', 'Measurement']\n",
        ").mark_area(\n",
        "    opacity=0.7,\n",
        "    interpolate='step'\n",
        ").encode(\n",
        "    alt.X('Measurement:Q', bin=alt.Bin(maxbins=50)),\n",
        "    alt.Y('count()', stack=None),\n",
        "    alt.Color('Centrality:N')\n",
        ")\n",
        "\n",
        "chart.configure_legend(\n",
        "    strokeColor='gray',\n",
        "    fillColor='#EEEEEE',\n",
        "    padding=10,\n",
        "    cornerRadius=10,\n",
        "    orient='top-right'\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_graphFile='weight_repo.graphml'\n",
        "with open(weight_graphFile, 'wb') as fOut:\n",
        "    nx.write_graphml(network_weight, fOut)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " # TODO: Analysis Approach\n",
        "### RUBRIC: Approach (20%)\n",
        "\n",
        " - [ ] The approach is an appropriate method to take to solve the problem or answer the analytical question. \n",
        " - [ ] Approach taken goes beyond using the tools provided in class. \n",
        " - [ ] Team justifies and explains their approach well. Approach includes data collected and techniques used.\n",
        "\n",
        " ## TODO: Describe what analysis you performed to answer the questions\n",
        " ## TODO: What type of sentiment analysis did you do?  Briefly explain your rationale for doing it as such.\n",
        " ## TODO: What type of topic modelling did you do?  Again, briefly explain your rationale for your approach.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### RUBRIC: Analysis/result & Discussion (20%)\n",
        "\n",
        " - [ ] Problem solving: The solution solves the problem well and all the success criteria are satisfied. \n",
        " - [ ] Team is able to provide analytical and/or empirical evidence of this.\n",
        " - [ ] Answering analytical question & Analysis component: \n",
        "   - [ ] Excellent discussion of results that answers the question proposed \n",
        "     - [ ] or contributes towards solving the problem. \n",
        "   - [ ] Conclusions are supported by analytical and/or empirical evidence. \n",
        "   - [ ] All success criteria are answered.\n",
        "\n",
        " # TODO: Analysis & Insights\n",
        " ## TODO: Present your analysis, to answer the questions \n",
        " ## TODO: Present and discuss your insights\n",
        " ## TODO: Use plots, tables, example of prints, visualisation, word clouds etc that supports your analysis and insights\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  Conclusion\n",
        "##  Provide a short conclusion about your entity, analysis and what you found\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Team Management & Mentoring\n",
        "\n",
        "- [ ] Use timesheets\n",
        "- [ ] Set up regular meetings within your team\n",
        "- [ ] We've setup a slack channel\n",
        "- [ ] [Github repo](https://github.com/tgrrr/data-science-social-network-analysis)\n",
        "- [ ] Meet lecturers 3 times before assignment due"
      ],
      "metadata": {}
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}